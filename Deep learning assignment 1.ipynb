{"metadata":{"language_info":{"name":"python","version":"3.7.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Q1. What is the function of a summation junction of a neuron? What is threshold activation function?","metadata":{},"id":"3742de98-fb62-4055-811c-34664c0ab359"},{"cell_type":"markdown","source":"Ans - The neuron act as a summation junction because all the individual entity which are nothing but multiplication of the input and the weights are         all summed up and fed to the neuron. It can also be called as linear combiner.\n      \n      The threshold activation function can be used as a switch for the neuron as below the threshold the neuron is deactivated and above the               threshold the neuron as activated.","metadata":{},"id":"ef05decb-d077-46c2-83ad-bbf71ac9e880"},{"cell_type":"markdown","source":"Q2. What is a step function? What is the difference of step function with threshold function?","metadata":{},"id":"5369110f-8bc3-4f90-89ab-974bb61a570e"},{"cell_type":"markdown","source":"Ans - The step function is generally used when we are working with perceptron. It produces the output as 1 when the value of x is grater than 0, and 0 when the value of x is less than 0. \n\nThe step function is very different than the threshold function because the step function is not differentiable at 0 and all the neural network work on the principle of back propagation and differentiation of step function only produces 0 or 1, which are not prefferable in the neural network.\n\nThe main objective of the neural network is to learn the values of the weights and biases so that the model could produce a prediction as close as possible to the real value. In order to do this, as in many optimisation problems, we’d like a small change in the weight or bias to cause only a small corresponding change in the output from the network. By doing this, we can continuously tweaked the values of weights and bias towards resulting the best approximation. Having a function that can only generate either 0 or 1 (or yes and no), won't help us to achieve this objective.","metadata":{},"id":"60541c18-a91b-44c7-8804-cc2d23a75d30"},{"cell_type":"markdown","source":"Q3. Explain the McCulloch–Pitts model of neuron.","metadata":{},"id":"6e7da785-a617-4364-8b6a-c560eb0e0793"},{"cell_type":"markdown","source":"Ans - The McCulloch–Pitts model is a basic neural network that generate either 0 or 1 as output. The input variable are multiplied with there respective weights.\n\nNow the weights are of two types excitatory or inhibitory. Now we need to sum the inputs. If an input is one, and is excitatory in nature, it added one. If it was one, and was inhibitory, it subtracted one from the sum. This is done for all inputs, and a final sum is calculated.\nNow, if this final sum is less than some value (which you decide, say T), then the output is zero. Otherwise, the output is a one.\n\nThis the basic working of the McCulloch–Pitts model.","metadata":{},"id":"882d65c4-4939-4a4c-bd0f-a739a1a0e5a4"},{"cell_type":"markdown","source":"Q4. Explain the ADALINE network model.","metadata":{},"id":"8193b457-0767-4753-9fda-a3db535abf2b"},{"cell_type":"markdown","source":"Ans - The ADALINE model is the advancement in the perceptron. It uses the continous activation function which led to the introduction of the gradient descent \napproach used to minimize the cost function by adjusting the weights of the neurons.\n\nThe key difference between Adaline and Perceptron is that the weights in Adaline are changed on a linear activation function rather than the unit step\nfunction.Coming back Adaline, this cost function is the Sum of squared errors (SSE) between the calculated outcome by the activation function and the \ntrue class label.","metadata":{},"id":"2034a928-f40c-494f-9263-e90cbc021719"},{"cell_type":"markdown","source":"Q5. What is the constraint of a simple perceptron? Why it may fail with a real-world data set?","metadata":{},"id":"f4ce38dd-c315-4f67-8276-2602196fc06a"},{"cell_type":"markdown","source":"Ans - The major limitation of the simple perceptron is that it can omnly produce the output in either 0 or 1. The second drawback is that it can only \nwork on the linearly seperable data, it cannot classify the non-linear relationship between the data. \n\nThus in todays real world dataset, there is hardly any dataset that will have linear dependencies, all data are non-linear, thus the simple perceptron will fail to classify all the classes and will not produce the results.","metadata":{},"id":"64f80cda-0f26-4d73-bcfb-0d2db5d56a95"},{"cell_type":"markdown","source":"Q6. What is linearly inseparable problem? What is the role of the hidden layer?","metadata":{},"id":"61e72378-78bc-4c1a-a460-571a99b17641"},{"cell_type":"markdown","source":"Ans - Decision problems that are not linearly separable, they cannot be solved using a linear decision boundary,these are called linearly inseparable problems. XOR is a linearly inseparable problem.\n\nThe hidden layer are present between the input and the output layer. They take take the weighted inputs as there input and apply activation function to produce the output which can be fed to the next hiden layer or to the output layer. \n\nThe activation function used in the hidden layer should be a linearly differentiable function, so that there is no vanishing gradient problem for training of large neural network.\n","metadata":{},"id":"de3a2306-941e-4f6c-92ca-c2dcc168c863"},{"cell_type":"markdown","source":"Q7. Explain XOR problem in case of a simple perceptron.","metadata":{},"id":"bf16e159-25f8-499b-aa21-7b6cbf47063a"},{"cell_type":"markdown","source":"Ans - A \"single-layer\" perceptron can't implement XOR. The reason is because the classes in XOR are not linearly separable. You cannot draw a straight\nline to separate the points (0,0),(1,1) from the points (0,1),(1,0).\n\nIt can be proved by the foolwing - \n\nXOR is where if one is 1 and other is 0 but not both.\n\nNeed:\n    \n1.w1 + 0.w2 cause a fire, i.e. >= t\n\n0.w1 + 1.w2 >= t\n\n0.w1 + 0.w2 doesn't fire, i.e. < t\n\n1.w1 + 1.w2 also doesn't fire, < t\n\n\nw1 >= t\n\nw2 >= t\n\n0 < t\n\nw1+w2 < t\n\nContradiction.Then the weights can be greater than t yet adding them is less than t, but t > 0 stops this.","metadata":{},"id":"b6c49be6-7888-4b90-aea5-41c53bffce59"},{"cell_type":"markdown","source":"Q8. Design a multi-layer perceptron to implement A XOR B.","metadata":{},"id":"1ec0407c-2ea9-4323-a1bb-ce6d82b20218"},{"cell_type":"code","source":"import numpy as np\nimport random\nnp.random.seed(0)\n\ndef sigmoid(x): # Returns values that sums to one.\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(sx):\n    # See https://math.stackexchange.com/a/1225116\n    return sx * (1 - sx)\n\n# Cost functions.\ndef cost(predicted, truth):\n    return truth - predicted\n\n# Shuffle the order of the inputs\ntemp = list(zip(X, Y))\nrandom.shuffle(temp)\nxor_input_shuff, xor_output_shuff = map(np.array, zip(*temp))\n\n# Lets split the data to 90-10. \ntrain_split = int(len(X) / 100 * 90)\nX_train = xor_input_shuff[:train_split]\nY_train = xor_output_shuff[:train_split]\n\nX_test = xor_input_shuff[train_split:]\nY_test = xor_output_shuff[train_split:]\n\n# Define the shape of the weight vector.\nnum_data, input_dim = X_train.shape\n# Lets set the dimensions for the intermediate layer.\nhidden_dim = 5\n# Initialize weights between the input layers and the hidden layer.\nW1 = np.random.random((input_dim, hidden_dim))\n\n# Define the shape of the output vector. \noutput_dim = len(Y_train.T)\n# Initialize weights between the hidden layers and the output layer.\nW2 = np.random.random((hidden_dim, output_dim))\n\nnum_epochs = 2000\nlearning_rate = 0.03\n\nfor epoch_n in range(num_epochs):\n    layer0 = X_train\n    # Forward propagation.\n    \n    # Inside the perceptron, Step 2. \n    layer1 = sigmoid(np.dot(layer0, W1))\n    layer2 = sigmoid(np.dot(layer1, W2))\n\n    # Back propagation (Y -> layer2)\n    \n    # How much did we miss in the predictions?\n    layer2_error = cost(layer2, Y_train)\n    # In what direction is the target value?\n    # Were we really close? If so, don't change too much.\n    layer2_delta = layer2_error * sigmoid_derivative(layer2)\n\n    \n    # Back propagation (layer2 -> layer1)\n    # How much did each layer1 value contribute to the layer2 error (according to the weights)?\n    layer1_error = np.dot(layer2_delta, W2.T)\n    layer1_delta = layer1_error * sigmoid_derivative(layer1)\n    \n    # update weights\n    W2 +=  learning_rate * np.dot(layer1.T, layer2_delta)\n    W1 +=  learning_rate * np.dot(layer0.T, layer1_delta)\naccurate = 0\nfor x, y in zip(X_test, Y_test):\n    layer1_prediction = sigmoid(np.dot(W1.T, x)) # Feed the unseen input into trained W.\n    prediction = layer2_prediction = sigmoid(np.dot(W2.T, layer1_prediction)) # Feed the unseen input into trained W.\n    print(x, [int(_x > 0.5) for _x in x], int(prediction > 0.5), y)\n    accurate += int(prediction > 0.5) == y","metadata":{},"execution_count":null,"outputs":[],"id":"5f949a1e-2ab3-42a7-a7e4-893ce4e88cda"},{"cell_type":"markdown","source":"Q9. Explain the single-layer feed forward architecture of ANN.","metadata":{},"id":"b366078d-a06b-44e7-a87b-ef7d3d3ddf6d"},{"cell_type":"markdown","source":"Ans - In this type of network, we have only two layers, i.e. input layer and output layer but the input layer does not count because no computation is performed in this layer.\nOutput Layer is formed when different weights are applied on input nodes and the summation effect per node is taken. After this, the neurons collectively give the output layer to compute the output signals.","metadata":{},"id":"9b76414d-ec96-42cf-8c7d-15427c766730"},{"cell_type":"markdown","source":"Q10. Explain the competitive network architecture of ANN.","metadata":{},"id":"cffdd59e-cc1c-43c0-9083-2b4eea648703"},{"cell_type":"markdown","source":"Ans - Input Layers, Neurons, and Weights –\n\nThe input layer. A neuron is the basic unit of a neural network. They receive input from an external source or other nodes. Each node is connected with another node from the next layer, and each such connection has a particular weight. Weights are assigned to a neuron based on its relative importance against other inputs.\n\nWhen all the node values are multiplied with their weight and summarized, it generates a value for the first hidden layer. Based on the summarized value, layer has a predefined activation function that determines whether or not this node will be activated.\n\nHidden Layers and Output Layer –\n\nThe layer that are between the input and output layer is known as the hidden layer. The main computation of a Neural Network takes place in the hidden layers. So, the hidden layer takes all the inputs from the input layer and performs the necessary calculation to generate a result. This result is then forwarded to the output layer so that the user can view the result of the computation.\nThus the ANN architecture consist of mainly three things input layer, hidden layer and output layer.","metadata":{},"id":"b94b5e43-57ea-4aa2-a29b-c6265eda312d"},{"cell_type":"markdown","source":"Q11. Consider a multi-layer feed forward neural network. Enumerate and explain steps in the backpropagation algorithm used to train the network.","metadata":{},"id":"6343bab4-3e83-4b7c-91b0-a076b8cc760b"},{"cell_type":"markdown","source":"Ans - If our feed forward network contain only one hidden layer then,there are three steps to solve the problem:\n    1. Computing the output, y.\n    2. Backpropagation of errors, i.e., between output and hidden layer, hidden and input layer.\n    3. Updating weights.\n\nNow in backpropagation -\n 1.  Calculating the error between output and hidden layer.\n 2.  Calculating the error between hidden and input layer.\n ","metadata":{},"id":"de6c8c79-0725-40a1-92df-b6b8e4dc745c"},{"cell_type":"markdown","source":"Q12. What are the advantages and disadvantages of neural networks?","metadata":{},"id":"01a4b869-07eb-42fe-a75f-5c004db56262"},{"cell_type":"markdown","source":"Ans - Advantages of neural network are -\n      1. Neural network learning methods are quite robust to noise in the training data.\n      2. If one neuron is corrupted, it does not prevent it from generating output.This feature makes the networks fault-tolerant.\n      3. Neural networks have numerical strength that can perform more than one job at the same time(parallel processing).\n     Disadvantages of Neural Networks - \n      1. Neural Networks require processors with parallel processing power, by their structure as with increase in the number of hidden layer the parameters rise exponentially as well.\n      2. There is no specific rule for determining the architecture of neural networks, its a hit and trial method which can cost lot of resources. \n      3. Neural network can work with numerical information.","metadata":{},"id":"e7b08b74-8495-4cfb-a111-273033258bd7"},{"cell_type":"markdown","source":"Q13. Write short notes on any two of the following:\n\n1. Biological neuron\n2. ReLU function\n3. Single-layer feed forward ANN\n4. Gradient descent\n5. Recurrent networks","metadata":{},"id":"4aa4cb3d-b542-47cb-b0ef-f77079d5ab9d"},{"cell_type":"markdown","source":"Ans -\n1. Biological neuron - \n   The biological neuron acts as messanger in the human body it transmits elctrical pulse signal from one neuron to other. The neuron mainly have three parts - \n    1. Dendrites\n    2. cell body\n    3. Axon\n   The dentrites recieve signal from other neurons and provide it to the cell body and then the cell body pass it to the axon, then axon transmits these signal to other neurons, this      cycle is repeated again for next process.\n\n\n2. Relu function - \n   ReLu is a non-linear activation function that is used in multi-layer neural networks or deep neural networks. This function can be represented as:\n    f(x)= max(0,x) , where x = an input value\n\n     \nAccording to equation 1, the output of ReLu is the maximum value between zero and the input value. An output is equal to zero when the input value is negative and the input value when the input is positive. Thus, we can rewrite equation 1 as follows:\n\n\nf(x)={\n       0, if x<0,\n       \n       x, if x>0,\n      \n     } , where x = an input value\n   The values below zero are neglected by the relu function.\n   Some advantage of using relu function over tanh and sigmoid is that it solves the vanishing gradient as well as exploding gradient problem. Its not zero centric function as compared to other activation functions.\n   \n","metadata":{},"id":"dc272df7-4e69-4735-a3c5-62a4c86fdf5a"}]}